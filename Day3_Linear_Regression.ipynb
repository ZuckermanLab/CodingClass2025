{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY/5m38uCBn0ZwyA8LCGlX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZuckermanLab/CodingClass2025/blob/main/Day3_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6TvQNKPCAwE"
      },
      "outputs": [],
      "source": [
        "# Day 3: Correlation and Linear Regression\n",
        "! pip install cptac\n",
        "import cptac\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "import cptac.utils as ut"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is correlation?**\n",
        "\n",
        "Correlation shows how two things are related. In this case:\n",
        "- Do students who study more get better scores?\n",
        "- If the dots go up from left to right, the correlation is positive.\n",
        "- If the dots go down from left to right, the correlation is negative.\n",
        "- If the dots are random with no clear pattern, there's no correlation.\n",
        "\n",
        "Try changing y = y2 or y = y3 in the code to see the difference!"
      ],
      "metadata": {
        "id": "3eve_RwIMoDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "x = np.array([1, 2, 3, 4, 5, 6, 7, 8])  # Hours studied\n",
        "\n",
        "# Three different relationships\n",
        "# Positive correlation: more studying = higher score\n",
        "y1 = [19, 30, 36, 30, 36, 45, 48, 55]\n",
        "\n",
        "# Negative correlation: more studying = lower score\n",
        "y2 = [15, 12, 10, 7, 5, 4, 2, 1]\n",
        "\n",
        "# No correlation: scores are random\n",
        "y3 = [7, 3, 8, 1, 6, 9, 2, 5]"
      ],
      "metadata": {
        "id": "_JI3Xy48MecF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose one of the y-values to plot\n",
        "y = y1  # Try y2 or y3 for different results\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel(\"Hours Studied\")\n",
        "plt.ylabel(\"Test Score\")\n",
        "plt.title(\"Study Time vs. Test Score\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#Calculate and print the Pearson correlation coefficient\n",
        "r_value, _ = pearsonr(x, y)\n",
        "print(f\"Pearson correlation (r): {r_value:.2f}\")"
      ],
      "metadata": {
        "id": "8toEeos5Mj3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Pearson's r?**\n",
        "\n",
        "Pearson's r is a number between -1 and 1 that tells us how strong the linear relationship is:\n",
        "- r close to 1: strong positive correlation (as x increases, y increases)\n",
        "- r close to -1: strong negative correlation (as x increases, y decreases)\n",
        "- r close to 0: no clear linear relationship\n",
        "\n",
        "Pearson's r can be calculated in as follows:\n",
        "\n",
        "$$\n",
        "r = \\frac{\\text{cov}(x, y)}{\\sigma_x \\cdot \\sigma_y}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\text{cov}(x, y)=\\sum (x_i - \\bar{x})(y_i - \\bar{y})/N$ is the covariance between x and y\n",
        "- $\\sigma_x=\\sqrt{\\sum (x_i - \\bar{x})^2/N}$ is the standard deviation of x\n",
        "- $\\sigma_y=\\sqrt{\\sum (y_i - \\bar{y})^2/N}$ is the standard deviation of y\n",
        "- $N$ is the number of data points\n",
        "- $\\bar{x}$ and $\\bar{y}$ are the mean values of $x$ and $y$, respectively\n",
        "\n",
        "The code above prints the Pearson r value so you can see how strong the connection is!\n"
      ],
      "metadata": {
        "id": "5lKjYy6MNr2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is linear regression?**\n",
        "\n",
        "Linear regression finds the best-fitting straight line through a set of points.\n",
        "It helps us make predictions: If we know how many hours someone studied, we can estimate their test score.\n",
        "\n",
        "The equation of the line is:\n",
        "\n",
        "$$ y = mx + b $$\n",
        "\n",
        "where:\n",
        "- $y$ is the predicted value (e.g. test score)\n",
        "- $x$ is the input (e.g. hours studied)\n",
        "- $m$ is the slope (how much y increases when x increases by 1)\n",
        "- $b$ is the intercept (the value of y when x = 0)\n",
        "\n",
        "We use this line to make predictions and understand trends!\n",
        "\n",
        "**How is a \"good fit\" defined?**\n",
        "\n",
        "A good fit means the line is close to the actual data points. One way to measure this is by using the sum of squared errors.\n",
        "\n",
        "$$\n",
        "\\text{squared errors} = \\sum (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $y_i$ is the actual value\n",
        "- $\\hat{y}_i$ is the predicted value from the line\n",
        "\n",
        "A smaller squared error means the predictions are close to the real values, so the line fits the data well.\n",
        "\n",
        "We'll use `scikit-learn` to fit the line and plot it below."
      ],
      "metadata": {
        "id": "OWw5ck68QgYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Linear Regression with Error Visualization\n",
        "y=y1\n",
        "\n",
        "# Reshape x to 2D array for scikit-learn\n",
        "X = x.reshape(-1, 1)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get slope and intercept\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "\n",
        "# Make predicted y values based on the line\n",
        "y_pred = slope * x + intercept\n",
        "\n",
        "# Plot data points and regression line\n",
        "plt.scatter(x, y, label=\"Data Points\")\n",
        "plt.plot(x, y_pred, color='red', label=\"Best Fit Line\")\n",
        "\n",
        "# Draw vertical residual lines\n",
        "for xi, yi, ypi in zip(x, y, y_pred):\n",
        "    plt.plot([xi, xi], [yi, ypi], color='blue', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.xlabel(\"Hours Studied\")\n",
        "plt.ylabel(\"Test Score\")\n",
        "plt.title(\"Linear Regression with Error Visualization\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Slope: {slope:.2f}\")\n",
        "print(f\"Intercept: {intercept:.2f}\")\n",
        "\n",
        "squared_error = np.sum((y - y_pred) ** 2)\n",
        "print(f\"Squared Error: {squared_error:.2f}\")"
      ],
      "metadata": {
        "id": "80nYKnvVTFRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict score for 10 hours of study based on linear regression\n",
        "#x_new = 10\n",
        "#y_new = _____ * x_new + _____\n",
        "#print(f\"Predicted score for 10 hours: {y_new:.1f}\")"
      ],
      "metadata": {
        "id": "ajeQatVlMlKm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What about more than one input?**\n",
        "\n",
        "Sometimes, one input isn’t enough. For example, test scores might depend on both how much someone **studied** *and* how much they **slept**.\n",
        "\n",
        "In this case, we can still use linear regression — we just add more input variables.\n",
        "\n",
        "The equation becomes:\n",
        "\n",
        "$$\n",
        "y = a \\cdot x_1 + b \\cdot x_2 + c\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $x_1$ is the first input (e.g. hours studied)\n",
        "- $x_2$ is the second input (e.g. hours slept)\n",
        "- $a$ and $b$ are the slopes for each input\n",
        "- $c$ is the intercept\n",
        "\n",
        "This is called **multiple linear regression**. The model finds the best-fitting plane (instead of a line) in 3D space.\n",
        "\n",
        "We can still measure how well the model fits using the **sum of squared errors** — just like before, but now predictions depend on multiple inputs.\n",
        "\n",
        "We’ll fit this using `scikit-learn` and print the equation and error below.\n"
      ],
      "metadata": {
        "id": "JSw9GJFNPdLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input features\n",
        "x1 = x  # study hours\n",
        "x2 = np.array([4, 5, 7, 5, 5, 7, 9, 8])  # sleep hours\n",
        "\n",
        "y = y1 # test scores\n",
        "\n",
        "# Stack input features\n",
        "X = np.column_stack((x1, x2)) # What does this do?\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Coefficients and intercept\n",
        "a, b = model.coef_\n",
        "c = model.intercept_\n",
        "\n",
        "print(f\"Slope for x1 (study hours): {a:.2f}\")\n",
        "print(f\"Slope for x2 (sleep hours): {b:.2f}\")\n",
        "print(f\"Intercept: {c:.2f}\")"
      ],
      "metadata": {
        "id": "YzA0EyeGdonH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Meshgrid for plane\n",
        "x1_grid, x2_grid = np.meshgrid(np.linspace(min(x1), max(x1), 10),\n",
        "                               np.linspace(min(x2), max(x2), 10))\n",
        "y_grid = a * x1_grid + b * x2_grid + c\n",
        "\n",
        "# Create single row with 3 subplots\n",
        "fig = plt.figure(figsize=(18, 5))\n",
        "gs = fig.add_gridspec(1, 3)\n",
        "\n",
        "# ---- Subplot 1: 3D Regression ----\n",
        "ax1 = fig.add_subplot(gs[0, 0], projection='3d')\n",
        "ax1.scatter(x1, x2, y, color='blue', label='Data Points')\n",
        "ax1.plot_surface(x1_grid, x2_grid, y_grid, alpha=0.4, color='red')\n",
        "ax1.set_xlabel(\"Study Hours\")\n",
        "ax1.set_ylabel(\"Sleep Hours\")\n",
        "ax1.set_zlabel(\"Test Score\", labelpad=-1)\n",
        "ax1.set_title(\"2D Linear Regression\")\n",
        "ax1.view_init(elev=30, azim=225)\n",
        "\n",
        "# ---- Subplot 2: x1 vs y ----\n",
        "x1_range = np.linspace(min(x1), max(x1), 100)\n",
        "x2_mean = np.mean(x2)\n",
        "y_x1 = a * x1_range + b * x2_mean + c\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.plot(x1_range, y_x1, color='red', label=f'x2 fixed at mean = {x2_mean:.2f}')\n",
        "ax2.scatter(x1, y, color='blue', label='Data Points')\n",
        "ax2.set_xlabel(\"Study Hours (x1)\")\n",
        "ax2.set_ylabel(\"Test Score\")\n",
        "ax2.set_title(\"Effect of Study Hours\")\n",
        "ax2.grid(True)\n",
        "ax2.legend()\n",
        "\n",
        "# ---- Subplot 3: x2 vs y ----\n",
        "x2_range = np.linspace(min(x2), max(x2), 100)\n",
        "x1_mean = np.mean(x1)\n",
        "y_x2 = a * x1_mean + b * x2_range + c\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "ax3.plot(x2_range, y_x2, color='green', label=f'x1 fixed at mean = {x1_mean:.2f}')\n",
        "ax3.scatter(x2, y, color='blue', label='Data Points')\n",
        "ax3.set_xlabel(\"Sleep Hours (x2)\")\n",
        "ax3.set_ylabel(\"Test Score\")\n",
        "ax3.set_title(\"Effect of Sleep Hours\")\n",
        "ax3.grid(True)\n",
        "ax3.legend()\n",
        "\n",
        "# Final layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D0beNzYtFKsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare the squared errors of different fits. What do you conclude?\n",
        "\n",
        "# Model 1: only x1 (same as above)\n",
        "X1 = x1.reshape(-1, 1)\n",
        "model1 = LinearRegression()\n",
        "model1.fit(X1, y)\n",
        "y_pred1 = model1.predict(X1)\n",
        "squared_error1 = np.sum((y - y_pred1) ** 2)\n",
        "\n",
        "print(f\"Squared Error (only x1 - study hours): {squared_error1:.2f}\")\n",
        "\n",
        "# Model 2: only x2 (ToDo)\n",
        "\n",
        "\n",
        "# Model 3: both x1 and x2 (ToDo)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cK8Tc2XjeajP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e4a83d-fdd5-494f-d99c-9c4f2439856b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Squared Error (only x1 - study hours): 94.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Is all of this information really important?**\n",
        "\n",
        "Sometimes datasets contain redundant information or we are dealing with high-dimensional data and want to analyze patterns and reduce dimensionality.\n",
        "\n",
        "**Principal Component Analysis (PCA)** helps us do that. It finds new directions (called **principal components**) that capture the most variation in the data.\n",
        "\n",
        "For example, if we have two inputs — study hours and sleep hours — PCA finds the **main direction** students vary in. This is helpful for:\n",
        "\n",
        "- Visualizing high-dimensional data in 2D\n",
        "- Understanding which combinations of features matter most\n",
        "\n",
        "PCA creates new axes like:\n",
        "\n",
        "$$\n",
        "\\text{PC}_1 = w_1 \\cdot x_1 + w_2 \\cdot x_2\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $x_1$ and $x_2$ are the original features (study and sleep)\n",
        "- $w_1$ and $w_2$ are weights chosen to capture as much variation as possible\n",
        "\n",
        "We'll now apply PCA to our study/sleep data and plot the result.\n"
      ],
      "metadata": {
        "id": "89PtWFOdQO9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PCA analysis\n",
        "\n",
        "X = np.column_stack((x1, x2))\n",
        "\n",
        "# --- PCA Analysis ---\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# --- Linear Regression using PC1 ---\n",
        "PC1 = X_pca[:, 0].reshape(-1, 1)\n",
        "model = LinearRegression()\n",
        "model.fit(PC1, y)\n",
        "y_pred = model.predict(PC1)\n",
        "squared_error = np.sum((y - y_pred) ** 2)\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "\n",
        "# --- Plotting side by side ---\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Original data + PCA components\n",
        "axs[0].scatter(X[:, 0], X[:, 1], color='blue', label='Original Data')\n",
        "origin = np.mean(X, axis=0)\n",
        "\n",
        "for i, (length, vector) in enumerate(zip(pca.explained_variance_ / 2, pca.components_)):\n",
        "    arrow = length * vector\n",
        "    axs[0].arrow(origin[0], origin[1], arrow[0], arrow[1],\n",
        "                 head_width=0.2, color=f'C{i}', label=f'PC{i+1}')\n",
        "\n",
        "axs[0].set_xlabel(\"Study Hours\")\n",
        "axs[0].set_ylabel(\"Sleep Hours\")\n",
        "axs[0].set_title(\"PCA on Study vs Sleep Data\")\n",
        "axs[0].grid(True)\n",
        "axs[0].axis('equal')\n",
        "axs[0].legend()\n",
        "\n",
        "# Plot 2: PC1 vs Test Scores + Regression line\n",
        "axs[1].scatter(PC1, y, color='blue', label='Data')\n",
        "axs[1].plot(PC1, y_pred, color='red', label='Fit Line')\n",
        "axs[1].set_xlabel(\"First Principal Component (PC1)\")\n",
        "axs[1].set_ylabel(\"Test Score\")\n",
        "axs[1].set_title(\"Linear Regression Using PC1\")\n",
        "axs[1].grid(True)\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print explained variance and regression info\n",
        "for i, var in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1} explains {var:.2%} of the variance\")\n",
        "\n",
        "print(f\"Squared Error: {squared_error:.2f}\")"
      ],
      "metadata": {
        "id": "CcvHnkyhIvUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loading Lung cancer genomics data\n",
        "\n",
        "#Load proteomics and transcriptomics data for Lung Cancer dataset\n",
        "proteomics = cptac.Luad().get_proteomics('bcm')\n",
        "transcriptomics = cptac.Luad().get_transcriptomics('bcm')\n",
        "\n",
        "# Reduce multi-index structure for both\n",
        "proteomics = ut.reduce_multiindex(proteomics, levels_to_drop='Database_ID')\n",
        "transcriptomics = ut.reduce_multiindex(transcriptomics, levels_to_drop='Database_ID')\n",
        "\n",
        "# Find common columns (genes) between proteomics and transcriptomics\n",
        "common_columns = proteomics.columns.intersection(transcriptomics.columns)\n",
        "\n",
        "# Align proteomics and transcriptomics by common columns (genes)\n",
        "proteomics = proteomics[common_columns]\n",
        "transcriptomics = transcriptomics[common_columns]\n",
        "\n",
        "# Remove proteins with NaN values and adjust transcriptomics accordingly\n",
        "proteomics = proteomics.dropna(axis=1, how='any')\n",
        "transcriptomics = transcriptomics[proteomics.columns]\n",
        "\n",
        "# Find common indices (patients) between proteomics and transcriptomics\n",
        "common_index = proteomics.index.intersection(transcriptomics.index)\n",
        "proteomics = proteomics.loc[common_index]\n",
        "transcriptomics = transcriptomics.loc[common_index]\n",
        "\n",
        "# Compute Q1 and Q3 for transcriptomics\n",
        "q1_trans = transcriptomics.quantile(0.25)\n",
        "q3_trans = transcriptomics.quantile(0.75)\n",
        "\n",
        "# Remove genes with Q1 = 0 (to avoid division issues in CQV)\n",
        "valid_trans = q1_trans != 0\n",
        "q1_trans = q1_trans[valid_trans]\n",
        "q3_trans = q3_trans[valid_trans]\n",
        "\n",
        "# Compute CQV for transcriptomics\n",
        "cqv_trans = (q3_trans - q1_trans) / (q3_trans + q1_trans)\n",
        "\n",
        "# Compute Q1 and Q3 for proteomics\n",
        "q1_prot = proteomics.quantile(0.25)\n",
        "q3_prot = proteomics.quantile(0.75)\n",
        "\n",
        "# Remove genes with Q1 = 0\n",
        "valid_prot = q1_prot != 0\n",
        "q1_prot = q1_prot[valid_prot]\n",
        "q3_prot = q3_prot[valid_prot]\n",
        "\n",
        "# Compute CQV for proteomics\n",
        "cqv_prot = (q3_prot - q1_prot) / (q3_prot + q1_prot)\n",
        "\n",
        "# Drop any NaN or infinite CQV values\n",
        "cqv_trans = cqv_trans.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "cqv_prot = cqv_prot.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "# Find genes that have CQVs in both datasets\n",
        "common_genes = cqv_trans.index.intersection(cqv_prot.index)\n",
        "\n",
        "# Combine CQVs (e.g., average or min or max depending on your use case)\n",
        "combined_cqv = (cqv_trans[common_genes] + cqv_prot[common_genes]) / 2\n",
        "\n",
        "# Select top 100 genes by combined CQV\n",
        "top_genes = combined_cqv.sort_values(ascending=False).head(100).index\n",
        "\n",
        "# Filter proteomics and transcriptomics to top genes\n",
        "proteomics = proteomics[top_genes]\n",
        "transcriptomics = transcriptomics[top_genes]\n",
        "\n",
        "# Print top genes in a readable format\n",
        "print(\"\\nTop 100 Most Variable Genes:\\n\")\n",
        "\n",
        "for i, gene in enumerate(top_genes, 1):\n",
        "    print(f\"{gene:20}\", end='\\n' if i % 5 == 0 else ' ')\n"
      ],
      "metadata": {
        "id": "TgEvPCB7RUR_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task: Perform linear regression to predict proteomics from transcriptomics for one gene\n",
        "\n",
        "# Choose a target gene (the protein we want to predict)\n",
        "gene = top_genes[50]  # You can change this!\n",
        "\n",
        "# Extract transcript and protein values\n",
        "x = transcriptomics[gene].values  # predictor\n",
        "y = proteomics[gene].values # target\n",
        "\n",
        "X = x.reshape(-1, 1)\n",
        "\n",
        "# Step 1: Compute and print the pearson r value\n",
        "# r_value, _ = ______\n",
        "# print(f\"Pearson r: {r_value:.2f}\")\n",
        "\n",
        "# Step 2: Create and fit a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# TODO: Fit the model to X and y\n",
        "# model.______(____, ____)\n",
        "\n",
        "# Step 3: Predict the proteomics values\n",
        "# y_pred = model.________(____)\n",
        "\n",
        "# Step 4: Plot original data and regression line\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(x, y, label=\"Data\", color='blue')\n",
        "# plt.plot(x, ____, color='red', label=\"Fit Line\")  # TODO: plot the prediction line\n",
        "plt.xlabel(\"Transcript Level\")\n",
        "plt.ylabel(\"Protein Level\")\n",
        "plt.title(f\"Linear Regression for {gene}\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Compute and print the squared error\n",
        "# error = ______\n",
        "# print(f\"Squared Error: {error:.2f}\")"
      ],
      "metadata": {
        "id": "I4NFfk1FY02v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task: Predict the same protein level using two or more Transcript levels and compare the results\n",
        "\n",
        "# Choose a target gene (the protein we want to predict)\n",
        "gene = top_genes[50]  # You can change this!\n",
        "\n",
        "# Choose predictor genes (the transcripts we use to predict protein levels)\n",
        "predictor_genes = top_genes[:3]  # You can change this list!\n",
        "\n",
        "# Step 1: Extract X (predictors) and y (target)\n",
        "X = transcriptomics[predictor_genes].values  # shape: (samples, predictors)\n",
        "y = proteomics[gene].values                  # shape: (samples,)\n",
        "\n",
        "# Step 2: Compute and print Pearson r values between each predictor and the target\n",
        "\n",
        "print(f\"Target protein: {gene}\")\n",
        "for pred_gene in predictor_genes:\n",
        "    print('')\n",
        "    # r_value, _ = ________(transcriptomics[pred_gene], y)\n",
        "    # print(f\"Pearson r ({pred_gene} vs {gene}): {r_value:.2f}\")\n",
        "\n",
        "# Step 3: Fit the multivariate linear regression model\n",
        "model = LinearRegression()\n",
        "# TODO: Fit the model to X and y\n",
        "# model.______(____, ____)\n",
        "\n",
        "# Step 4: Predict protein values\n",
        "# y_pred = model.________(____)\n",
        "\n",
        "# Step 5: Plot actual vs predicted protein levels\n",
        "plt.figure(figsize=(6, 4))\n",
        "# plt.scatter(____, ____, color='green', alpha=0.7)\n",
        "# plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='Perfect Prediction')\n",
        "plt.xlabel(\"Actual Protein Level\")\n",
        "plt.ylabel(\"Predicted Protein Level\")\n",
        "plt.title(f\"Multivariate Regression for {gene}\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 6: Compute and print squared error\n",
        "# squared_error = ((____ - ____)**2).sum()\n",
        "# print(f\"Total Squared Error (multi-input): {squared_error:.2f}\")"
      ],
      "metadata": {
        "id": "z3ZS755iiyxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bonus Task: Do a PCA of the Top 100 genes and use the first 10 PC's to predict the Protein Level"
      ],
      "metadata": {
        "id": "anFW9B4Elj3V"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}