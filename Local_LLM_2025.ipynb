{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMB28s9sRCnxqZbajb4o1dF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZuckermanLab/CodingClass2025/blob/main/Local_LLM_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Local Large Language Models"
      ],
      "metadata": {
        "id": "JZouez3MQiJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first\n",
        " thing we need to do is get an API access key from huggingface.com. After creating an acount on Hugging Face, go to https://huggingface.co/google/gemma-2b to accept the T&C for the Gemma model. Next, go to Settings > Access Tokens and create a new \"Read\" token. Keep this page up as we will need to re-enter this token later."
      ],
      "metadata": {
        "id": "vDZLuN8rjpkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "00m_CNR3UNUo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\n",
        "model = model.to('cuda')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TKKrV5eOQTTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"hello world ðŸ¤—\")"
      ],
      "metadata": {
        "id": "dpFMkQeT2xgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(223603)"
      ],
      "metadata": {
        "id": "3nXEtuNWLxNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  return tokenizer.encode(text, return_tensors='pt').to('cuda')\n",
        "\n",
        "def detokenize(tokens):\n",
        "  return tokenizer.decode(tokens)"
      ],
      "metadata": {
        "id": "jKoh_LgJLZyN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can test out the `generate` method of our model. For this, we will pass in our tokenized input prompt (`prompt`) and also specify the maximum amount of new tokens we want the model to generate. We also want to set `do_sample` to `False` so that we get the same output each time."
      ],
      "metadata": {
        "id": "j4o09frs2Zaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of Oregon? Answer with one word only.\""
      ],
      "metadata": {
        "id": "mdIVNT8W3ML6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = model.generate(tokenize(prompt), max_new_tokens=25, do_sample=False)"
      ],
      "metadata": {
        "id": "fjYfn6bMT0mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(detokenize(output_tokens[0]))"
      ],
      "metadata": {
        "id": "L8Rcsb8CTNCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examining the next-token distribution"
      ],
      "metadata": {
        "id": "ahgPdbapW-kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenize(\"What is the capital of Oregon? Answer with one word only.<end_of_turn><start_of_turn>\\n\\n\")"
      ],
      "metadata": {
        "id": "ruf4kYL6tFVt",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each time we pass our input through the model (the \"forward pass\"), the output we get is called the \"logits\", which is a set of scores for each token ID in the model's vocabulary, which for Gemma is 262,144 different tokens."
      ],
      "metadata": {
        "id": "0vIevLf1Axfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.forward(tokens) #model.forward() is equivalent to model.predict() from yesterday"
      ],
      "metadata": {
        "id": "goE7w22VeP1Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output['logits'].shape"
      ],
      "metadata": {
        "id": "vAN4-2V_eWly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6PJwurp0CGnr"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_probs = softmax(\n",
        "    output['logits'][0][-1]\n",
        "    .cpu()\n",
        "    .detach()\n",
        "    .numpy()\n",
        "    )"
      ],
      "metadata": {
        "id": "35EmZEF_HA1z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_word_probs = np.array(\n",
        " sorted([[i,l] for i,l in enumerate(word_probs)], key=lambda x: x[1], reverse=True)\n",
        ")[:50]\n",
        "fig, ax = plt.subplots(figsize=(20,2))\n",
        "plt.bar(range(len(top_word_probs)), top_word_probs[:,1])\n",
        "plt.xticks(\n",
        "    range(len(top_word_probs)),\n",
        "    [tokenizer.decode(int(t)) for t in top_word_probs[:,0]],\n",
        "    rotation=75,\n",
        "    fontsize=8\n",
        ")\n",
        "\n",
        "plt.xlabel('next token')\n",
        "plt.ylabel('Probability')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T4yW7SOFGS2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(tokenizer.decode(int(t)), int(t)) for t in top_word_probs[:,0]]"
      ],
      "metadata": {
        "id": "kVpWlLk2H8PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(output['logits'][0][-1].argmax())"
      ],
      "metadata": {
        "id": "rJr4rjm0v7F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,token in enumerate(tokens[0]):\n",
        "  print([tokenizer.decode(t) for t in tokens[0][:i+1]], tokenizer.decode(output['logits'][0][i].argmax()))"
      ],
      "metadata": {
        "id": "ss1YU__Hek5o",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the Generate() method ourselves"
      ],
      "metadata": {
        "id": "0Ua5MdTS3JKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reality when we call the `generate()` method of our model, we are really running the model multiple times in a for loop. At each step we are taking the next word predicted by the model, adding it to our input, and running the model again."
      ],
      "metadata": {
        "id": "n4wGrzK4_z0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So to replicate the functionality of the `generate()` method, we just need to call the `forward()` method in a loop, take the argmax of the logits for the last token in the input, add that new token ID to the end of our input, and repeat until we reach the maximum number of new tokens."
      ],
      "metadata": {
        "id": "cKKdinGpB-XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def get_next_token(logits):\n",
        "  return torch.tensor([[logits[0][-1].argmax()]]).to('cuda')\n",
        "\n",
        "def concatenate_tokens(tokens, next_token):\n",
        "  return torch.cat([tokens, next_token], dim=1)"
      ],
      "metadata": {
        "id": "qBrSXBs4E5rt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_generate(model, prompt, max_new_tokens=250):\n",
        "  #tokenize prompt\n",
        "  input_tokens =\n",
        "\n",
        "  for i in range():\n",
        "    #pass input tokens through the model to get output tokens\n",
        "    output =\n",
        "\n",
        "    #get the most likely next token\n",
        "    next_token =\n",
        "\n",
        "    #concatenate next predicted token to output\n",
        "    input_tokens =\n",
        "\n",
        "    #check to see if we reached the end of the sentence\n",
        "    if detokenize(next_token[0]) == '<end_of_turn>':\n",
        "      break\n",
        "\n",
        "  return detokenize(input_tokens[0])"
      ],
      "metadata": {
        "id": "qgt4Ye2CFuG_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of Oregon?\"\n",
        "answer = model_generate(model, prompt)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "1TSE1FTnF72t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ShIgpGjrbYWY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}